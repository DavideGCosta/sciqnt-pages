"use strict";(self.webpackChunksciqnt_pages=self.webpackChunksciqnt_pages||[]).push([[6093],{9734:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>l});var s=t(4848),i=t(8453);const a={},r="3. Clustering",o={id:"documentation/strategies/financial-news/clustering",title:"3. Clustering",description:"SciQnt.com makes things easier. Try it\xa0out.",source:"@site/docs/documentation/strategies/01-financial-news/03-clustering.md",sourceDirName:"documentation/strategies/01-financial-news",slug:"/documentation/strategies/financial-news/clustering",permalink:"/sciqnt-pages/docs/documentation/strategies/financial-news/clustering",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/documentation/strategies/01-financial-news/03-clustering.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"2. Sentence Embeddings",permalink:"/sciqnt-pages/docs/documentation/strategies/financial-news/sentence-embeddings"},next:{title:"Strategy 2",permalink:"/sciqnt-pages/docs/category/strategy-2"}},c={},l=[{value:"Background",id:"background",level:3},{value:"Introduction",id:"introduction",level:3},{value:"Model of Choice: SBERT\u200a-\u200aall-MiniLM-L6-v2",id:"model-of-choice-sbert-all-minilm-l6-v2",level:3},{value:"High Quality",id:"high-quality",level:5},{value:"Efficiency",id:"efficiency",level:5},{value:"Versatility and Ease of\xa0Use",id:"versatility-and-ease-ofuse",level:5},{value:"Pre-Trained on Diverse\xa0Data",id:"pre-trained-on-diversedata",level:5},{value:"Accuracy in Semantic Matching",id:"accuracy-in-semantic-matching",level:5},{value:"Sentence Embeddings using\xa0SBERT",id:"sentence-embeddings-usingsbert",level:3},{value:"A few insights based on the results:",id:"a-few-insights-based-on-the-results",level:4},{value:"Considerations in Interpretation",id:"considerations-in-interpretation",level:4},{value:"Optimized Embedding Processor with GPU Support for Sentence Transformers",id:"optimized-embedding-processor-with-gpu-support-for-sentence-transformers",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h3:"h3",h4:"h4",h5:"h5",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"3-clustering",children:"3. Clustering"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.sciqnt.com",children:"SciQnt.com"})," makes things easier. Try it\xa0out."]}),"\n",(0,s.jsx)("div",{class:"alert alert-block alert-info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("b",{children:"Disclaimer:"})," Our strategies are tailored to specific input data and desired outcomes, which means the results presented here may not fully apply to your unique situation. The primary goal is to expose you to various approaches and key considerations. Additionally, please understand that the stock market is highly complex and constantly evolving; predicting returns is challenging, and no strategy is foolproof. Moreover, we want to be transparent about our use of state-of-the-art Large Language Models (LLMs) in our research. While these tools are invaluable, particularly in the writing process, they cannot independently conduct the research. They are used to augment our efforts, not replace them."]})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"background",children:"Background"}),"\n",(0,s.jsx)(n.p,{children:"In the fast-paced world of financial news and behavioral finance, each piece of information contains both fundamental and non-fundamental elements. The challenge lies in sifting through the overwhelming noise and massive influx of data that surpass human processing capabilities. But what if we could identify patterns within this data? Better yet, what if we could use computers to do it for us? Today, we are fortunate to have access to open-source models that make text comprehensible to machines. This process, known as \u2018Sentence Embeddings,\u2019 has revolutionized AI and deep learning. By converting textual data into meaningful numerical representations, sentence embeddings empower us to discern market sentiment, spot emerging trends, and predict financial movements with unprecedented efficiency and accuracy."}),"\n",(0,s.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Once we have collected financial news, the next step is to automate its interpretation. This means transforming the raw data for downstream applications like detecting trending topics through clustering or performing sentiment analysis. At the heart of this transformation is 'Sentence Embeddings,' which convert complex textual data into meaningful numerical vectors. This article delves into how to leverage pre-trained models to transform your data, enabling you to extract deeper insights and make more informed decisions in the dynamic world of finance."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#TESTTt\xce\xcdafszft\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"%%capture\n# Install the necessary packages\n!pip install plotly pandas numpy matplotlib seaborn sentence-transformers sklearn\n\nimport warnings\n# Suppress all warnings - remove it when testing the notebook to see the warnings\nwarnings.simplefilter(\"ignore\")\n\nimport os\n# Get theme from environment variable or default to 'plotly'\ntheme = os.getenv('PLOTLY_THEME', 'plotly')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Plotting Script\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Example embeddings and titles\nembeddings = [\n    [0.8, -0.6, 0.3],\n    [0.7, -0.5, 0.4],\n    [0.8, -0.5, 0.3],\n    [0.1, 0.9, -0.2],\n    [-0.3, 0.4, 0.9],\n    [0.0, 0.8, -0.3]\n]\n\ntitles = [\n    \"The stock market is experiencing a significant downturn.\",\n    \"The economy is showing signs of recovery.\",\n    \"There is a major decline in the stock market today.\",\n    \"The weather is sunny and pleasant.\",\n    \"Apple releases its new iPhone model.\",\n    \"It's raining heavily in the city.\"\n]\n\n# Convert vectors to a DataFrame\ndata = {\n    'Title': titles,\n    'X': [vec[0] for vec in embeddings],\n    'Y': [vec[1] for vec in embeddings],\n    'Z': [vec[2] for vec in embeddings]\n}\n\ndf = pd.DataFrame(data)\n\n# Create a 3D scatter plot\nfig = px.scatter_3d(df, x='X', y='Y', z='Z', color='Title')\n\n# Update marker size and remove text labels\nfig.update_traces(marker=dict(size=5), selector=dict(mode='markers'))\n\n# Set the layout of the plot\nfig.update_layout(\n    scene=dict(\n        xaxis_title='PCA Component 1',\n        yaxis_title='PCA Component 2',\n        zaxis_title='PCA Component 3'\n    ),\n    title='3D Visualization of Financial News Titles w/ Manual Embeddings',\n    legend_title='Titles',\n    margin=dict(l=0, r=0, b=0, t=40),\n    template=theme \n)\n\nfig.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:'This example illustrates how sentence embeddings map sentences to corresponding vectors in a numerical space. Although we have manually assigned these vectors for demonstration purposes, in practice, pre-trained models handle this complex task. Sentence embeddings enable us to mathematically derive relationships between sentences. Observing the chart above, we see that "The stock market is experiencing a significant downturn." is very close to "There is a major decline in the stock market today." These sentences share a similar context and sentiment, hence their proximity. In contrast, "The economy is showing signs of recovery." is somewhat further away, reflecting its different sentiment while still being related to economic context. This demonstrates how embeddings capture nuanced relationships between sentences, allowing for sophisticated text analysis.'}),"\n",(0,s.jsx)(n.p,{children:"This is a quite modest explanation of sentence embeddings. In case you're unfamiliar with it and want to understand more about it take a look at the resources:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://sbert.net",children:"Sentence Transformers Documentation - SBERT"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/index",children:"Hugging Face Transformers Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://www.tensorflow.org/hub/tutorials",children:"TenserFlow Tutorials"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf",children:"Google Universal Sentence Encoder"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://fasttext.cc/docs/en/support.html",children:"FastText Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://spacy.io/api/doc",children:"spaCy Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"http://jalammar.github.io/illustrated-bert/",children:"The Illustrated BERT, ELMo, and co. by Jay Alammar"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://colab.research.google.com/drive/132E8CG4fRT0Yudxs5eYIm9xD69o242Uw?usp=sharing%3A",children:"Universal Sentence Encoder Colab"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1908.10084",children:"Sentence Embeddings using Siamese BERT-Networks"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.h3,{id:"model-of-choice-sbert-all-minilm-l6-v2",children:["Model of Choice: SBERT\u200a-\u200a",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"SBERT (Sentence-BERT)"}),", especially the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})})," model, stands out as the go-to option for clustering financial news titles. Its blend of efficiency, accuracy, and deep semantic understanding makes it superior to many alternatives. Here's why ",(0,s.jsxs)(n.strong,{children:["SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})]})," often takes the lead:"]}),"\n",(0,s.jsx)(n.h5,{id:"high-quality",children:"High Quality"}),"\n",(0,s.jsxs)(n.p,{children:["SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})," is exceptional at grasping the nuances of sentence-level semantics. This model is highly effective in understanding and grouping similar financial news titles, where subtle contextual differences can significantly influence clustering outcomes. While USE and DistilBERT offer good semantic understanding, they don't quite match SBERT's fine-tuning for sentence similarity, particularly in the complex world of financial news."]}),"\n",(0,s.jsx)(n.h5,{id:"efficiency",children:"Efficiency"}),"\n",(0,s.jsxs)(n.p,{children:["Designed to be lightweight, SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})," shines in large-scale clustering tasks without sacrificing embedding quality. It balances model size and performance perfectly, making it ideal for real-time analysis and handling large datasets. In contrast, DistilBERT, though smaller and faster than full BERT, doesn't capture sentence-level semantics as effectively as SBERT. USE is efficient but tends to be bulkier compared to the sleek SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"}),"."]}),"\n",(0,s.jsx)(n.h5,{id:"versatility-and-ease-ofuse",children:"Versatility and Ease of\xa0Use"}),"\n",(0,s.jsx)(n.p,{children:"SBERT is easy to integrate with popular NLP libraries and comes with thorough documentation, making deployment and experimentation straightforward. While USE and DistilBERT are also user-friendly, SBERT's approach to sentence-level embeddings is more refined, making it particularly suited for tasks like clustering and semantic similarity."}),"\n",(0,s.jsx)(n.h5,{id:"pre-trained-on-diversedata",children:"Pre-Trained on Diverse\xa0Data"}),"\n",(0,s.jsxs)(n.p,{children:["SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})," is pre-trained on large, diverse datasets, ensuring it handles various types of financial news robustly. This extensive pre-training means it generalizes well across different contexts and terminologies found in financial titles. FastText and GloVe, in contrast, focus more on word-level embeddings and lack the sophisticated sentence-level training that SBERT excels at."]}),"\n",(0,s.jsx)(n.h5,{id:"accuracy-in-semantic-matching",children:"Accuracy in Semantic Matching"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})," model is built for highly accurate semantic matching, which is crucial for clustering news titles where context and sentiment are key. USE and DistilBERT also perform well in this area, but they don't quite reach the level of fine-tuning and optimization that SBERT provides specifically for sentence-level tasks."]}),"\n",(0,s.jsxs)(n.p,{children:["Here are some of the alternative options summarized, if you want to explore other models and comparison in performance check out the ",(0,s.jsx)(n.a,{href:"https://medium.com/r/?url=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fmteb%2Fleaderboard",children:"Massive Text Embedding Benchmark (MTEB) Leaderboard"}),"."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/bert",children:"BERT"})," (Bidirectional Encoder Representations from Transformers):"]}),"  BERT is a pre-trained language model developed by Google. It captures bidirectional context and has been widely used for various NLP tasks.\nPros: (High accuracy, Captures bidirectional context, Versatile for various NLP tasks)\nCons: (Computationally expensive, Slower inference time, Large memory footprint)\nExample Models: bert-base-uncased, bert-large-uncased"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/roberta",children:"RoBERTa"})," (Robustly optimized BERT approach):"]})," RoBERTa is an optimized version of BERT by Facebook AI, trained with larger mini-batches and longer sequences.\n\xa0Example Models: roberta-base, roberta-large"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/albert",children:"ALBERT"})," (A Lite BERT):"]})," ALBERT is a lighter version of BERT that reduces the number of parameters to improve training efficiency and scalability.\nExample Models: albert-base-v2, albert-large-v2"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers",children:"SBERT"})," (Sentence-BERT):"]})," SBERT is a modification of the BERT network to derive semantically meaningful sentence embeddings that can be compared using cosine similarity.\nExample Models: ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"}),", paraphrase-MiniLM-L12-v2"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder",children:"USE"})," (Universal Sentence Encoder):"]})," USE by Google provides versatile sentence embeddings optimized for a variety of tasks and languages.\nExample Models: universal-sentence-encoder, universal-sentence-encoder-large"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://nlp.stanford.edu/projects/glove/",children:"GloVe"})," (Global Vectors for Word Representation):"]})," GloVe is an unsupervised learning algorithm for obtaining vector representations for words, which can be used to create sentence embeddings by averaging word vectors.\xa0\nExample Models: glove.6B.300d, glove.840B.300d"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://fasttext.cc",children:"FastText"}),":"]}),"  FastText by Facebook AI is an extension of the Word2Vec model that uses subword information to create word vectors, which can be used to form sentence embeddings.\nExample Models: wiki-news-300d-1M, crawl-300d-2M"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/t5",children:"T5"})," (Text-to-Text Transfer Transformer):"]})," T5 by Google treats all NLP tasks as a text-to-text problem, which can be used to generate embeddings from sentence representations.\nExample Models: t5-small, t5-large"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/xlnet",children:"XLNet"}),":"]})," XLNet is an auto-regressive pre-trained language model that combines the best of both autoregressive and autoencoding language models.\n\u2022 Example Models: xlnet-base-cased, xlnet-large-cased"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/ernie",children:"ERNIE"})," (Enhanced Representation through Knowledge Integration):"]})," ERNIE by Baidu integrates knowledge graphs into the pre-training process to enhance the understanding of language context.\nExample Models: ernie-1.0, ernie-2.0"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/model_doc/distilbert",children:"DistilBERT"}),":"]})," DistilBERT is a smaller, faster, and lighter version of BERT that retains 97% of BERT's language understanding.\nExample Models: distilbert-base-uncased, distilbert-base-multilingual-cased"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"sentence-embeddings-usingsbert",children:"Sentence Embeddings using\xa0SBERT"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsxs)(n.strong,{children:["SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})]})," model, hosted by ",(0,s.jsx)(n.strong,{children:"Hugging Face"}),", simplifies the implementation of advanced semantic analysis, making it accessible even for complex tasks (a big win for us!). To get started, you only need to install the sentence-transformers library via pip, which provides easy access to the SentenceTransformer module. This module allows you to transform sentences into rich, meaningful vector representations that capture their semantic essence. In essence, a sentence transformer converts text into numerical vectors, enabling nuanced understanding and comparison of sentences. You can learn more about sentence transformers here."]}),"\n",(0,s.jsx)(n.p,{children:"Once the model is loaded, you can simply use model.encode to generate these embeddings, unlocking the potential to analyze and cluster text based on its meaning. This function turns your sentences into vectors that can be used to perform a wide range of downstream tasks, from clustering and classification to semantic search and beyond."}),"\n",(0,s.jsx)(n.p,{children:"Here's the implementation in python:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'%%capture\n\nfrom sentence_transformers import SentenceTransformer\n\n# Example financial news titles\ntitles = [\n    "The stock market is experiencing a significant downturn.",\n    "The stock market saw a sharp decline today.",\n    "Major losses hit the stock market.",\n    "Tech stocks rally as market rebounds from losses.",\n    "Technology shares surge following market recovery.",\n    "Recovery drives tech stocks to new highs.",\n    "Federal Reserve signals possible interest rate hike.",\n    "Fed hints at increasing interest rates soon.",\n    "Interest rate hike likely as suggested by the Federal Reserve.",\n    "Bitcoin reaches new all-time high amid market optimism.",\n    "Market optimism pushes Bitcoin to record high.",\n    "Bitcoin hits unprecedented levels with market cheer.",\n    "Oil prices surge due to supply chain disruptions.",\n    "Supply chain issues cause a spike in oil prices.",\n    "Oil prices increase sharply as supply chains falter."\n]\n\n# Load the SBERT model\nmodel = SentenceTransformer(\'sentence-transformers/all-MiniLM-L6-v2\')\n\n# Compute embeddings\nembeddings = model.encode(titles)\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"embeddings"}),' are dense numerical vectors that capture the semantic meaning and contextual relationships of sentences. For instance, the embedding of a sentence like "The stock market is experiencing a significant downturn." might be represented as a high-dimensional vector such as ',(0,s.jsx)(n.code,{children:"sentence1 -> [0.45, -0.40, 0.17,\xa0...];"}),"\xa0. Each embedding generated using ",(0,s.jsxs)(n.strong,{children:["SBERT ",(0,s.jsx)(n.a,{href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",children:"all-MiniLM-L6-v2"})]})," consists of 384 dimensions. These dimensions collectively encode various aspects of the sentence's meaning."]}),"\n",(0,s.jsx)(n.p,{children:"But as humans, we don't naturally communicate in 384-dimensional numerical vectors. So, how do we verify that the embeddings truly capture the context and meaning of our sentences in a way that's useful? There are two straightforward ways to do this:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Comparing Embedding Similarity:"})," Similar sentences should have embeddings that are close to each other. We can measure this similarity to see if the embeddings reflect the semantic relationships between sentences."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport plotly.graph_objects as go\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate cosine similarity\nsimilarity_matrix = cosine_similarity(embeddings)\n\n# Mask the lower triangle and the diagonal of the similarity matrix\nmask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\nmasked_sim_matrix = np.ma.masked_array(similarity_matrix, mask)\n\n# Create the heatmap\nfig = go.Figure(data=go.Heatmap(\n    z=masked_sim_matrix,\n    x=titles,\n    y=titles,\n    colorscale='Greens',\n    zmin=0,\n    zmax=1,\n    colorbar=dict(title='Cosine Similarity'),\n    hoverongaps=False,\n))\n\n# Update layout\nfig.update_layout(\n    title='Cosine Similarity of Financial News Titles',\n    xaxis=dict(tickangle=90, tickfont=dict(size=8)),\n    yaxis=dict(tickfont=dict(size=8)),\n    height=1000,\n    margin=dict(l=100, r=100, t=100, b=100),\n    template=theme\n)\n\nfig.show()\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visualization:"})," As we demonstrated earlier, visualizing these embeddings can provide insights. Similar embeddings should cluster together in charts, showing that they capture the underlying context effectively. This way, we can visually validate that the embeddings represent the nuances of the text."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport plotly.graph_objects as go\nfrom sklearn.decomposition import PCA\n\n# Reduce dimensions (vector length from 384 to 3) to 3D for visualization using PCA\npca = PCA(n_components=3)  # PCA used for simplicity\nreduced_embeddings = pca.fit_transform(embeddings)\n\n# Create 3D scatter plot\nfig = go.Figure()\n\nfor i, embedding in enumerate(reduced_embeddings):\n    fig.add_trace(go.Scatter3d(\n        x=[embedding[0]],\n        y=[embedding[1]],\n        z=[embedding[2]],\n        mode='markers',  # Ensure markers are used\n        marker=dict(size=5),  # Adjust the size as needed\n        name=titles[i]  # Add the title to the legend\n    ))\n\n# Update layout\nfig.update_layout(\n    scene=dict(\n        xaxis_title='PCA Component 1',\n        yaxis_title='PCA Component 2',\n        zaxis_title='PCA Component 3'\n    ),\n    title='3D Visualization of Financial News Titles Using SBERT',\n    legend_title_text='Titles',\n    margin=dict(l=0, r=0, b=0, t=40),\n    template=theme\n)\n\nfig.show()\n"})}),"\n",(0,s.jsx)(n.h4,{id:"a-few-insights-based-on-the-results",children:"A few insights based on the results:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Clustering of Similar Titles:"}),' The plot effectively shows how SBERT captures and differentiates between semantically similar financial news titles. Titles discussing similar events, such as the stock market downturn, are closely clustered together. For instance, "The stock market is experiencing a significant downturn.", "The stock market saw a sharp decline today.", and "Major losses hit the stock market." are grouped closely. This clustering indicates that SBERT has effectively captured their semantic similarity, even though the phrasings differ. Similarly, titles about tech stock rallies, interest rate hikes, Bitcoin\'s market performance, and oil prices form distinct clusters. For example, titles like "Tech stocks rally as market rebounds from losses.", "Technology shares surge following market recovery.", and "Recovery drives tech stocks to new highs." are grouped together, reflecting their similar contexts. This clustering pattern demonstrates SBERT\'s ability to understand and encode the semantic content of these news titles accurately.']}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Distinct Clusters:"}),' Titles about different events are spread apart. Titles discussing interest rate hikes by the Federal Reserve, such as "Federal Reserve signals possible interest rate hike.", "Fed hints at increasing interest rates soon.", and "Interest rate hike likely as suggested by the Federal Reserve.", form a distinct cluster. This separation from those discussing the stock market or Bitcoin indicates clear differentiation in context and subject matter. Similarly, titles about oil prices, like "Oil prices surge due to supply chain disruptions.", "Supply chain issues cause a spike in oil prices.", and "Oil prices increase sharply as supply chains falter.", also form a distinct and clear cluster. These groupings highlight how SBERT can differentiate between various financial events, even when they are reported differently.']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"considerations-in-interpretation",children:"Considerations in Interpretation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cosine similarity captures overall semantic closeness but might miss finer nuances such as tone or specific entity relationships."}),"\n",(0,s.jsx)(n.li,{children:'High similarity scores suggest semantic similarity, but small differences in scores might not always be meaningful. It\'s crucial to set appropriate thresholds for what constitutes "similar" in your context.'}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"optimized-embedding-processor-with-gpu-support-for-sentence-transformers",children:"Optimized Embedding Processor with GPU Support for Sentence Transformers"}),"\n",(0,s.jsxs)(n.p,{children:["Here you have a Python class, ",(0,s.jsx)(n.code,{children:"EmbeddingsProcessor"}),", that manages the efficient computation and caching of sentence embeddings using the SentenceTransformer library. It dynamically selects the optimal device (NVIDIA CUDA, Apple MPS, AMD ROCm, or CPU) for processing, normalizes embeddings, and caches them to reduce redundant computations. This facilitates scalable and performance-optimized embedding generation."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\nimport pickle\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Dict\n\nclass EmbeddingsProcessor:\n    def __init__(self, model: SentenceTransformer, cache_path: str, device: str = None):\n        """\n        Initializes the EmbeddingsProcessor with a SentenceTransformer model,\n        a path for caching embeddings, and an optional device specification.\n\n        Parameters:\n        model (SentenceTransformer): The SentenceTransformer model used for generating embeddings.\n        cache_path (str): Path to store the cache of embeddings.\n        device (str): Preferred device for computation (\'cuda\', \'mps\', \'cpu\', or None).\n                      If None, the best available device will be used.\n        """\n        self.model = model\n        self.cache_path = cache_path\n        self.device = self._get_device(device)  # Determine the best available device\n        self.embeddings_cache = self._load_embeddings_cache()  # Load cached embeddings if available\n\n    def _get_device(self, preferred_device: str = None) -> torch.device:\n        """\n        Determines the best available device for computation.\n\n        Parameters:\n        preferred_device (str): The preferred device (optional).\n\n        Returns:\n        torch.device: The best available device.\n        """\n        if preferred_device:\n            # Attempt to use the preferred device\n            device = torch.device(preferred_device)\n            if device.type == \'cuda\' and torch.cuda.is_available():\n                return device\n            elif device.type == \'mps\' and torch.backends.mps.is_available():\n                return device\n            elif device.type == \'rocm\' and torch.cuda.is_available():\n                return device  # ROCm is detected as CUDA\n        # Default to the best available device\n        if torch.cuda.is_available():\n            return torch.device(\'cuda\')\n        elif torch.backends.mps.is_available():\n            return torch.device(\'mps\')\n        else:\n            return torch.device(\'cpu\')\n\n    def _load_embeddings_cache(self) -> Dict[str, torch.Tensor]:\n        """\n        Loads embeddings cache from the specified file path.\n\n        Returns:\n        dict: The loaded embeddings cache or an empty dictionary if the cache does not exist.\n        """\n        if os.path.exists(self.cache_path):\n            # Load the cache if the file exists\n            with open(self.cache_path, \'rb\') as f:\n                return pickle.load(f)\n        return {}  # Return an empty dictionary if the cache file does not exist\n\n    def _save_embeddings_cache(self):\n        """\n        Saves the current embeddings cache to the specified file path.\n        """\n        with open(self.cache_path, \'wb\') as f:\n            pickle.dump(self.embeddings_cache, f)\n\n    def _compute_embeddings(self, corpus: List[str]) -> torch.Tensor:\n        """\n        Computes and caches embeddings for a given corpus of texts.\n\n        Parameters:\n        corpus (list): A list of texts to compute embeddings for.\n\n        Returns:\n        torch.Tensor: A tensor of normalized embeddings.\n        """\n        embeddings = []\n        new_embeddings = {}\n        for text in corpus:\n            if text not in self.embeddings_cache:\n                # Compute embedding if it\'s not already cached\n                embedding = self.model.encode(text, convert_to_tensor=True)\n                new_embeddings[text] = embedding.cpu()  # Store the embedding on CPU to save GPU memory\n            else:\n                new_embeddings[text] = self.embeddings_cache[text]\n            embeddings.append(new_embeddings[text])\n\n        if new_embeddings:\n            # Update the cache if new embeddings were computed\n            self.embeddings_cache.update(new_embeddings)\n            self._save_embeddings_cache()\n\n        # Stack embeddings into a single tensor\n        embeddings_tensor = torch.stack(embeddings)\n        embeddings_tensor = embeddings_tensor.to(self.device)  # Move the tensor to the desired device\n        # Normalize the embeddings by their norms\n        norms = embeddings_tensor.norm(p=2, dim=1, keepdim=True)\n        normalized_embeddings = embeddings_tensor.div(norms)\n        return normalized_embeddings\n\n    def process_batch(self, corpus: List[str]) -> torch.Tensor:\n        """\n        Processes a batch of texts and returns their embeddings.\n\n        Parameters:\n        corpus (list): A list of texts to process.\n\n        Returns:\n        torch.Tensor: A tensor of embeddings for the given texts.\n        """\n        embeddings = self._compute_embeddings(corpus)\n        return embeddings\n\n# Example usage of the EmbeddingsProcessor\n# Load a SentenceTransformer model\nmodel = SentenceTransformer(\'sentence-transformers/all-MiniLM-L6-v2\')\n# Path to store the embeddings cache\ncache_path = \'embeddings_cache.pkl\'\n# Initialize the processor\nprocessor = EmbeddingsProcessor(model, cache_path)\n\n# Example corpus of texts\ncorpus = titles # Using the financial news titles from the previous example\n# Process the corpus to get embeddings\nembeddings = processor.process_batch(corpus)\nprint(corpus[0], embeddings[0][0:10]) # you can see here that for the first title, SBERT has generated a 384-dimensional embedding (only 10 dimensions are shown here to avoid a long output)\n'})}),"\n",(0,s.jsx)(n.p,{children:"The stock market is experiencing a significant downturn. tensor([ 0.0315, -0.0482,  0.0491,  0.0356, -0.0831,  0.0144, -0.0546,  0.0748,\n0.0136, -0.0191], device='mps:0')"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const i={},a=s.createContext(i);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);